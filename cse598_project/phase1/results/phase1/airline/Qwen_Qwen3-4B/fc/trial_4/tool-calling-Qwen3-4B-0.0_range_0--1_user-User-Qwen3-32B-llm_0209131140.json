[
  {
    "task_id": 21,
    "reward": 0.0,
    "info": {
      "error": "litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 500, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set\n"
    },
    "traj": [],
    "trial": 0
  },
  {
    "task_id": 9,
    "reward": 0.0,
    "info": {
      "error": "litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 500, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set\n"
    },
    "traj": [],
    "trial": 0
  },
  {
    "task_id": 3,
    "reward": 0.0,
    "info": {
      "error": "litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 500, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set\n"
    },
    "traj": [],
    "trial": 0
  },
  {
    "task_id": 20,
    "reward": 0.0,
    "info": {
      "error": "litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Error code: 400 - {'error': {'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 500, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: OpenAIException - \"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set\n"
    },
    "traj": [],
    "trial": 0
  },
  {
    "task_id": 12,
    "reward": 0.0,
    "info": {
      "error": "litellm.InternalServerError: InternalServerError: OpenAIException - Connection error.",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1002, in request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1034, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 554, in exception_type\n    raise InternalServerError(\nlitellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: OpenAIException - Connection error.\n"
    },
    "traj": [],
    "trial": 0
  },
  {
    "task_id": 2,
    "reward": 0.0,
    "info": {
      "error": "litellm.InternalServerError: InternalServerError: OpenAIException - Connection error.",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1002, in request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1034, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 554, in exception_type\n    raise InternalServerError(\nlitellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: OpenAIException - Connection error.\n"
    },
    "traj": [],
    "trial": 0
  },
  {
    "task_id": 10,
    "reward": 0.0,
    "info": {
      "error": "litellm.InternalServerError: InternalServerError: OpenAIException - Connection error.",
      "traceback": "Traceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1002, in request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 762, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 690, in completion\n    ) = self.make_sync_openai_chat_completion_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 237, in sync_wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 502, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 477, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/openai/_base_client.py\", line 1034, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2531, in completion\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 2503, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 773, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/run.py\", line 78, in _run\n    res = agent.solve(\n          ^^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/tau_bench/agents/tool_calling_agent.py\", line 53, in solve\n    res = completion(\n          ^^^^^^^^^^^\n  File \"/home/vgaduput/TLDR_AGENTIC_AI-1/cse598_project/phase1/scripts/custom_run.py\", line 38, in patched_completion\n    return original_completion(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1742, in wrapper\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/utils.py\", line 1563, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/main.py\", line 4242, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2378, in exception_type\n    raise e\n  File \"/home/vgaduput/.conda/envs/py312/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 554, in exception_type\n    raise InternalServerError(\nlitellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: OpenAIException - Connection error.\n"
    },
    "traj": [],
    "trial": 0
  }
]