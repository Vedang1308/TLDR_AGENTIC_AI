Starting vLLM server for User Agent (Qwen/Qwen3-32B as User-Qwen3-32B) on port 8001...
/Users/vedangavaghade/Desktop/CSE598 - Agentic AI/Project 1/TLDR_AGENTIC_AI/cse598_project/phase1/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
INFO 01-24 01:59:04 [__init__.py:216] Automatically detected platform cpu.
[1;36m(APIServer pid=50487)[0;0m INFO 01-24 01:59:05 [api_server.py:1896] vLLM API server version 0.10.2
[1;36m(APIServer pid=50487)[0;0m INFO 01-24 01:59:05 [utils.py:328] non-default args: {'port': 8001, 'model': 'Qwen/Qwen3-32B', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 8192, 'served_model_name': ['User-Qwen3-32B'], 'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192}
[1;36m(APIServer pid=50487)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=50487)[0;0m INFO 01-24 01:59:10 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM
[1;36m(APIServer pid=50487)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=50487)[0;0m WARNING 01-24 01:59:10 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.
[1;36m(APIServer pid=50487)[0;0m INFO 01-24 01:59:10 [__init__.py:1815] Using max model len 8192
[1;36m(APIServer pid=50487)[0;0m WARNING 01-24 01:59:10 [cpu.py:114] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.
[1;36m(APIServer pid=50487)[0;0m INFO 01-24 01:59:10 [arg_utils.py:1145] Chunked prefill is not supported for ARM and POWER and S390X CPUs; disabling it for V1 backend.
/Users/vedangavaghade/Desktop/CSE598 - Agentic AI/Project 1/TLDR_AGENTIC_AI/cse598_project/phase1/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
INFO 01-24 01:59:14 [__init__.py:216] Automatically detected platform cpu.
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:15 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:15 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=User-Qwen3-32B, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":2,"debug_dump_path":"","cache_dir":"","backend":"inductor","custom_ops":["none"],"splitting_ops":null,"use_inductor":true,"compile_sizes":null,"inductor_compile_config":{"enable_auto_functionalized_v2":false,"dce":true,"size_asserts":false,"nan_asserts":false,"epilogue_fusion":true},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":null,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:15 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[1;36m(EngineCore_DP0 pid=50505)[0;0m WARNING 01-24 01:59:15 [cpu.py:312] Pin memory is not supported on CPU.
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:16 [cpu_worker.py:70] Warning: NUMA is not enabled in this build. `init_cpu_threads_env` has no effect to setup thread affinity.
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:16 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:16 [cpu_model_runner.py:101] Starting to load model Qwen/Qwen3-32B...
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:16 [cpu.py:101] Using Torch SDPA backend.
[1;36m(EngineCore_DP0 pid=50505)[0;0m INFO 01-24 01:59:16 [weight_utils.py:348] Using model weights format ['*.safetensors']
./cse598_project/phase1/scripts/start_user_model.sh: line 24: 50487 Killed: 9               $PYTHON_EXEC -m vllm.entrypoints.openai.api_server --model $MODEL --served-model-name $ALIAS --trust-remote-code --port $PORT --dtype float16 --max-model-len 8192 --max-num-batched-tokens 8192 --tensor-parallel-size 1 --gpu-memory-utilization 0.45
 0.45
